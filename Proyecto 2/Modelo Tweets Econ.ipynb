{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1JAr12ZlRXIBl-EWvVNILgNeXBP1uWW2H","authorship_tag":"ABX9TyOF5XhBBwIELAUbObts/gGo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install numpy\n","!pip install pandas\n","!pip install re\n","!pip install transformers\n","!pip install stopwords\n","!pip install torch\n","!pip install torchtext"],"metadata":{"id":"axPTSrmofCCX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install pandas torch transformers nltk"],"metadata":{"id":"vgTVjL1b7YVC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lQp7CsJax3o1","executionInfo":{"status":"ok","timestamp":1697470941315,"user_tz":300,"elapsed":20454,"user":{"displayName":"Gabriel Orosco Jiménez","userId":"17287692603456795875"}},"outputId":"a2b28cea-95ec-47bc-e7a8-a7b2cbec775b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","rutaarchivo = \"/content/drive/MyDrive/Investigación/Data Entrenamiento.xlsx\"\n","rutaarchivo2 = \"/content/drive/MyDrive/Investigación/economic_dictionary.xlsx\"\n","DataEnt = pd.read_excel(rutaarchivo2)\n","DataEnt.head()"],"metadata":{"id":"N5YpIN-mxqNZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##BERT"],"metadata":{"id":"-GmESf1AsjYM"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from transformers import BertTokenizer, BertForSequenceClassification\n","import torch\n","\n","def entrenar_modelo_bert(data, text, sentiment, num_epochs=3):\n","    # Cargar el modelo BERT pre-entrenado y el tokenizador\n","    model_name = \"bert-base-uncased\"\n","    tokenizer = BertTokenizer.from_pretrained(model_name)\n","    model = BertForSequenceClassification.from_pretrained(model_name)\n","\n","    # Dividir los datos en entrenamiento y prueba\n","    X = data[text]\n","    y = data[sentiment]\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","    # Tokenizar los textos y convertirlos en tensores\n","    train_encodings = tokenizer(list(X_train), truncation=True, padding=True, return_tensors=\"pt\", max_length=512)\n","    test_encodings = tokenizer(list(X_test), truncation=True, padding=True, return_tensors=\"pt\", max_length=512)\n","\n","    # Crear tensores para las etiquetas\n","    train_labels = torch.tensor(list(y_train), dtype=torch.long)\n","    test_labels = torch.tensor(list(y_test), dtype=torch.long)\n","\n","    # Entrenar el modelo BERT\n","    train_data = torch.utils.data.TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n","    test_data = torch.utils.data.TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n","\n","    train_loader = torch.utils.data.DataLoader(train_data, batch_size=8)\n","    test_loader = torch.utils.data.DataLoader(test_data, batch_size=8)\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is available() else \"cpu\")\n","    model.to(device)\n","\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        for batch in train_loader:\n","            input_ids, attention_mask, labels = batch\n","            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            loss.backward()\n","            optimizer.step()\n","\n","    # Guardar el modelo entrenado\n","    model.save_pretrained(\"modelo_entrenado_bert\")\n","\n","    return \"Entrenamiento completado\"\n","\n","# Cargar los datos de los tweets desde un archivo Excel\n","rutaarchivo = \"/content/drive/MyDrive/Investigación/Data Entrenamiento.xlsx\" # Reemplaza con la ruta real de tu archivo Excel\n","tweets = pd.read_excel(rutaarchivo)\n","\n","# Entrenar el modelo BERT\n","resultado_entrenamiento = entrenar_modelo_bert(tweets, \"text\", \"sentiment\")\n","print(resultado_entrenamiento)\n","\n","\n"],"metadata":{"id":"c7mXO5QVy10q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from transformers import BertTokenizer, BertForSequenceClassification\n","import torch\n","\n","def cargar_modelo_y_predecir(textos_nuevos, modelo_entrenado):\n","    # Cargar el modelo BERT pre-entrenado y el tokenizador\n","    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","\n","    # Cargar el modelo entrenado\n","    model = BertForSequenceClassification.from_pretrained(modelo_entrenado)\n","\n","    # Tokenizar los nuevos textos\n","    encodings = tokenizer(list(textos_nuevos), truncation=True, padding=True, return_tensors=\"pt\", max_length=512)\n","\n","    # Realizar predicciones con el modelo\n","    model.eval()\n","    with torch.no_grad():\n","        input_ids = encodings['input_ids']\n","        attention_mask = encodings['attention_mask']\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        predicted_labels = torch.argmax(logits, dim=1)\n","\n","    return predicted_labels\n","\n","# Cargar los datos desde el archivo Excel \"data\" con una columna llamada \"tweet\"\n","data = pd.read_excel(\"/content/drive/MyDrive/Investigación/Data.xlsx\")\n","\n","# Extraer los tweets de la columna \"tweet\"\n","nuevos_textos = data[\"tweet\"].tolist()\n","\n","# Cargar el modelo entrenado y realizar predicciones\n","modelo_entrenado = \"/content/drive/MyDrive/Investigación/modelo_entrenado_bert\"  # Reemplaza con la ruta real de tu modelo entrenado\n","predicciones = cargar_modelo_y_predecir(nuevos_textos, modelo_entrenado)\n","\n","# Escalar los resultados al rango -1 a 1\n","predicciones_escaladas = (predicciones.float() / 1) * 2 - 1\n","\n","# Crear un DataFrame con los resultados\n","resultados_df = pd.DataFrame({\"Texto\": nuevos_textos, \"Sentimiento\": predicciones_escaladas.numpy()})\n","\n","# Guardar los resultados en un archivo Excel\n","resultados_df.to_excel(\"/content/drive/MyDrive/Investigación/resultados_sentimiento1.xlsx\", index=False)\n"],"metadata":{"id":"M6wQkE0Uy2q4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Redes"],"metadata":{"id":"cmd3AgLFz68O"}},{"cell_type":"code","source":["!pip install pandas numpy scikit-learn tensorflow tensorflow-cpu spacy"],"metadata":{"id":"_xUejJdhj9ei"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python -m spacy download es_core_news_sm"],"metadata":{"id":"LVW-N2ruobTN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import spacy\n","from google.colab import drive\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.optimizers import Adam\n","\n","# Montar Google Drive\n","drive.mount('/content/drive')\n","\n","# Ruta al archivo de datos en Google Drive\n","data_path = \"/content/drive/MyDrive/Investigación/Data Entrenamiento.xlsx\"\n","\n","# Cargar los datos\n","data = pd.read_excel(data_path)\n","X = data['text']  # Características (tweets)\n","y = data['sentiment']  # Etiquetas (sentimientos)\n","\n","# Cargar el modelo en español de spaCy\n","nlp = spacy.load(\"es_core_news_sm\")\n","\n","# Obtener la lista de palabras vacías en español de spaCy\n","stop_words_spanish = nlp.Defaults.stop_words\n","\n","# Convierte la lista de palabras vacías en español a una lista simple\n","stop_words_spanish = list(stop_words_spanish)\n","\n","# Preprocesamiento de texto\n","vectorizer = TfidfVectorizer(max_features=1000, stop_words=stop_words_spanish)\n","X = vectorizer.fit_transform(X)\n","X = X.toarray()  # Convertir la matriz dispersa en una matriz NumPy densa\n","\n","# Dividir los datos en conjuntos de entrenamiento y prueba\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Construir el modelo de red neuronal\n","model = Sequential()\n","model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n","model.add(Dense(64, activation='relu'))\n","model.add(Dense(1, activation='linear'))\n","\n","# Compilar el modelo\n","model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n","\n","# Entrenar el modelo\n","model.fit(X_train, y_train, epochs=10, batch_size=32)\n","\n","# Evaluar el modelo en el conjunto de prueba\n","loss = model.evaluate(X_test, y_test)\n","print(f'Error cuadrático medio en el conjunto de prueba: {loss:.2f}')\n","\n","# Utilizar el modelo para predecir sentimientos\n","predicted_sentiments = model.predict(X_test)\n","\n","# Guardar el modelo en un archivo\n","model.save(\"/content/drive/MyDrive/Investigación/modelo_analisis_sentimientored.h5\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SVucYapIkEJy","executionInfo":{"status":"ok","timestamp":1697564691518,"user_tz":300,"elapsed":3575,"user":{"displayName":"Gabriel Orosco Jiménez","userId":"17287692603456795875"}},"outputId":"24d7fb21-709f-477d-dd28-a05d720bdd60"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 1/10\n","1/1 [==============================] - 1s 649ms/step - loss: 0.7097\n","Epoch 2/10\n","1/1 [==============================] - 0s 10ms/step - loss: 0.6255\n","Epoch 3/10\n","1/1 [==============================] - 0s 11ms/step - loss: 0.5538\n","Epoch 4/10\n","1/1 [==============================] - 0s 9ms/step - loss: 0.4936\n","Epoch 5/10\n","1/1 [==============================] - 0s 10ms/step - loss: 0.4384\n","Epoch 6/10\n","1/1 [==============================] - 0s 9ms/step - loss: 0.3881\n","Epoch 7/10\n","1/1 [==============================] - 0s 9ms/step - loss: 0.3441\n","Epoch 8/10\n","1/1 [==============================] - 0s 9ms/step - loss: 0.3044\n","Epoch 9/10\n","1/1 [==============================] - 0s 11ms/step - loss: 0.2684\n","Epoch 10/10\n","1/1 [==============================] - 0s 12ms/step - loss: 0.2349\n","1/1 [==============================] - 0s 110ms/step - loss: 1.3648\n","Error cuadrático medio en el conjunto de prueba: 1.36\n","1/1 [==============================] - 0s 65ms/step\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Cargar los datos\n","data = pd.read_excel(\"/content/drive/MyDrive/Investigación/Data.xlsx\")\n","\n","## Agregar una columna sentiment con un valor predeterminado de neutral\n","#data[\"sentiment\"] = \"neutral\"\n","\n","# Guardar los datos\n","data.to_excel(\"/content/drive/MyDrive/Investigación/Data_con_Sentiment.xlsx\", index=False)\n"],"metadata":{"id":"7zqQLpgQtDDJ","executionInfo":{"status":"ok","timestamp":1697564699821,"user_tz":300,"elapsed":547,"user":{"displayName":"Gabriel Orosco Jiménez","userId":"17287692603456795875"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.optimizers import Adam\n","\n","# Cargar el modelo entrenado\n","loaded_model = load_model(\"/content/drive/MyDrive/Investigación/modelo_analisis_sentimientored.h5\")\n","\n","# Cargar los datos de entrada\n","#data = pd.read_excel(\"/content/drive/MyDrive/Investigación/Data_con_Sentiment.xlsx\")\n","data = pd.read_excel(\"/content/drive/MyDrive/Investigación/Data.xlsx\")\n","tweets = data['text']\n","\n","# Preprocesamiento de texto\n","X = vectorizer.transform(tweets).toarray()\n","\n","# Predecir el sentimiento de los tweets\n","predicted_sentiments = loaded_model.predict(X)\n","\n","# Agregar los puntajes de sentimiento al DataFrame original\n","data['sentiment_score'] = predicted_sentiments\n","\n","# Guardar el DataFrame resultante en un nuevo archivo Excel\n","data.to_excel(\"/content/drive/MyDrive/Investigación/Data_con_Puntajes_con_Sentiment.xlsx\", index=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"otb0F9wytE1s","executionInfo":{"status":"ok","timestamp":1697564705074,"user_tz":300,"elapsed":990,"user":{"displayName":"Gabriel Orosco Jiménez","userId":"17287692603456795875"}},"outputId":"019497c2-325e-4ee6-b21b-b1a57b6d901e"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 114ms/step\n"]}]}]}
