{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1POwm_5P6lw6RMzfkfnK_DCu381KKHcnO","authorship_tag":"ABX9TyPqbwbqCmpD0bN5EfiUiZM0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#CARGA DE DATOS"],"metadata":{"id":"wgfTRqzFVQZY"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QP2W4euWOrp-","executionInfo":{"status":"ok","timestamp":1694983655815,"user_tz":300,"elapsed":28210,"user":{"displayName":"Gabriel Orosco Jiménez","userId":"17287692603456795875"}},"outputId":"39640322-96eb-456f-9073-39bf512c3cca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","ruta_archivo = \"/content/drive/MyDrive/Investigación/Data Entrenamiento.xlsx\"\n","DataEnt = pd.read_excel(ruta_archivo)\n","DataEnt.head()"],"metadata":{"id":"LNHeVA_Y156p","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1694995282403,"user_tz":300,"elapsed":1463,"user":{"displayName":"Gabriel Orosco Jiménez","userId":"17287692603456795875"}},"outputId":"02c6888f-5b08-4e9c-8fd8-bcdb6a1a712a"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                               tweet  Date sentimiento  \\\n","0  Antes de Petro ya existía Colombia. Por eso es...   NaN    negativo   \n","1  OIT no apoya contenido de la reforma laboral d...   NaN    negativo   \n","2  El Presidente recibía una economía que apenas ...   NaN    positivo   \n","3  Reformas pensional y laboral del Gobierno Petr...   NaN         NaN   \n","4  Mientras el presidente Gustavo Petro le dice a...   NaN     neutro    \n","\n","   sentimiento1  \n","0          -1.0  \n","1          -0.9  \n","2           0.9  \n","3           0.6  \n","4           0.5  "],"text/html":["\n","  <div id=\"df-70e13179-a41f-4f45-8144-2ad4145685ea\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet</th>\n","      <th>Date</th>\n","      <th>sentimiento</th>\n","      <th>sentimiento1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Antes de Petro ya existía Colombia. Por eso es...</td>\n","      <td>NaN</td>\n","      <td>negativo</td>\n","      <td>-1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>OIT no apoya contenido de la reforma laboral d...</td>\n","      <td>NaN</td>\n","      <td>negativo</td>\n","      <td>-0.9</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>El Presidente recibía una economía que apenas ...</td>\n","      <td>NaN</td>\n","      <td>positivo</td>\n","      <td>0.9</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Reformas pensional y laboral del Gobierno Petr...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.6</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Mientras el presidente Gustavo Petro le dice a...</td>\n","      <td>NaN</td>\n","      <td>neutro</td>\n","      <td>0.5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-70e13179-a41f-4f45-8144-2ad4145685ea')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-70e13179-a41f-4f45-8144-2ad4145685ea button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-70e13179-a41f-4f45-8144-2ad4145685ea');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-77da3fa5-9066-4b9f-aa53-9c02d454cbb2\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-77da3fa5-9066-4b9f-aa53-9c02d454cbb2')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-77da3fa5-9066-4b9f-aa53-9c02d454cbb2 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["!pip install transformers\n","!pip install matplotlib\n","!pip install textblob"],"metadata":{"id":"3_oEZj5e02or","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694995315397,"user_tz":300,"elapsed":28841,"user":{"displayName":"Gabriel Orosco Jiménez","userId":"17287692603456795875"}},"outputId":"6144c8f9-770e-496d-9b06-2448700664ec"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n","  Downloading huggingface_hub-0.17.1-py3-none-any.whl (294 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.17.1 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.42.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n","Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.1)\n"]}]},{"cell_type":"markdown","source":["#Léxico\n"],"metadata":{"id":"E0RMhZJflZZo"}},{"cell_type":"markdown","source":["##VADER\n","(Valence Aware Dictionary and sEntiment Reasoner)\n","##SentiWordNet\n","(Base de datos léxica para asignar valores de polaridad de sentimiento)"],"metadata":{"id":"c4UHKathsVP5"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('sentiwordnet')\n"],"metadata":{"id":"gKm-tza9mr5m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694995327385,"user_tz":300,"elapsed":234,"user":{"displayName":"Gabriel Orosco Jiménez","userId":"17287692603456795875"}},"outputId":"d0090db4-fb0f-4ee6-86d3-7cf0a2b377fb"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n","[nltk_data]   Package sentiwordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["!pip install vaderSentiment"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2EoRbbhyQtSf","executionInfo":{"status":"ok","timestamp":1694995339457,"user_tz":300,"elapsed":7880,"user":{"displayName":"Gabriel Orosco Jiménez","userId":"17287692603456795875"}},"outputId":"553a9679-8040-424f-a4ee-6a2499e51cfd"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting vaderSentiment\n","  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m122.9/126.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2023.7.22)\n","Installing collected packages: vaderSentiment\n","Successfully installed vaderSentiment-3.3.2\n"]}]},{"cell_type":"code","source":["nltk.download('stopwords')\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NbjMReeYYc9o","executionInfo":{"status":"ok","timestamp":1694995339860,"user_tz":300,"elapsed":427,"user":{"displayName":"Gabriel Orosco Jiménez","userId":"17287692603456795875"}},"outputId":"e7e32777-1dbf-4a7c-de56-c09abddf2f83"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["import pandas as pd\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import sentiwordnet as swn\n","import nltk\n","import torch\n","import numpy as np\n","\n","# Descargar el recurso Spanish stopwords\n","nltk.download('stopwords')\n","\n","# Crear una instancia del analizador de sentimientos de Vader\n","analyzer = SentimentIntensityAnalyzer()\n","\n","# Cargar el tokenizador y el modelo preentrenado de BERT en español\n","tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')\n","model = BertForSequenceClassification.from_pretrained('dccuchile/bert-base-spanish-wwm-cased', num_labels=1)\n","\n","# Función para analizar el sentimiento de un texto usando Vader\n","def analyze_vader_sentiment(text):\n","    sentiment = analyzer.polarity_scores(text)\n","    return sentiment['compound']\n","\n","# Función para analizar el sentimiento de un texto usando SentiWordNet\n","def analyze_sentiwordnet_sentiment(text):\n","    tokens = word_tokenize(text.lower(), language='spanish')\n","    sentiment_score = 0\n","\n","    for token in tokens:\n","        synsets = list(swn.senti_synsets(token))\n","        if synsets:\n","            sentiment = synsets[0]\n","            sentiment_score += sentiment.pos_score() - sentiment.neg_score()\n","\n","    return sentiment_score\n","\n","# Ruta del archivo Excel\n","ruta_archivo = \"/content/drive/MyDrive/Investigación/Data Entrenamiento.xlsx\"\n","\n","# Leer el archivo Excel y seleccionar la columna \"tweet\"\n","data = pd.read_excel(ruta_archivo)\n","column_name = 'tweet'\n","\n","# Inicializar listas para almacenar las puntuaciones\n","vader_scores = []\n","sentiwordnet_scores = []\n","bert_scores = []\n","\n","for index, row in data.iterrows():\n","    text = row[column_name]\n","    vader_score = analyze_vader_sentiment(text)\n","    sentiwordnet_score = analyze_sentiwordnet_sentiment(text)\n","\n","    # Tokenizar y convertir el texto en tensor para BERT\n","    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n","    input_ids = inputs['input_ids']\n","    attention_mask = inputs['attention_mask']\n","\n","    # Realizar la predicción con BERT y asignar la puntuación de sentimiento\n","    with torch.no_grad():\n","        output = model(input_ids, attention_mask=attention_mask)[0]\n","\n","    # Agregar la puntuación de BERT a la lista\n","    bert_score = output.item()\n","    bert_scores.append(bert_score)\n","\n","    vader_scores.append(vader_score)\n","    sentiwordnet_scores.append(sentiwordnet_score)\n","\n","# Escalar las puntuaciones de BERT al rango de -1 a 1\n","min_bert_score = min(bert_scores)\n","max_bert_score = max(bert_scores)\n","bert_scores_normalized = [2 * (score - min_bert_score) / (max_bert_score - min_bert_score) - 1 for score in bert_scores]\n","\n","# Asignar las puntuaciones a las listas correspondientes\n","data['vader_sentiment_score'] = vader_scores\n","data['sentiwordnet_sentiment_score'] = sentiwordnet_scores\n","data['bert_sentiment_score'] = bert_scores_normalized\n","\n","# Guardar los resultados en un nuevo archivo Excel\n","result_file = \"/content/drive/MyDrive/Investigación/Resultados_Sentimiento.xlsx\"\n","data.to_excel(result_file, index=False)\n","\n","print(\"Resultados guardados en:\", result_file)\n","\n"],"metadata":{"id":"KhHOpwY9mZrH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694995452751,"user_tz":300,"elapsed":8538,"user":{"displayName":"Gabriel Orosco Jiménez","userId":"17287692603456795875"}},"outputId":"41d28c83-52fa-40ea-a3f2-52c6bc566a8c"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Resultados guardados en: /content/drive/MyDrive/Investigación/Resultados_Sentimiento.xlsx\n"]}]},{"cell_type":"markdown","source":["##EmoLex"],"metadata":{"id":"3XJHyskJsjmH"}},{"cell_type":"code","source":["import pandas as pd\n","from nltk.tokenize import word_tokenize\n","import nltk\n","\n","# Descargar el recurso Spanish stopwords\n","nltk.download('stopwords')\n","\n","# Función para cargar EmoLex\n","def load_emolex(emolex_file):\n","    emolex = {}\n","    with open(emolex_file, 'r') as file:\n","        for line in file:\n","            word, emotion, value = line.strip().split()\n","            if emotion == 'joy':\n","                emolex[word] = float(value)\n","    return emolex\n","\n","# Función para analizar el sentimiento de un texto usando EmoLex\n","def analyze_emolex_sentiment(text, emolex_lexicon):\n","    tokens = word_tokenize(text.lower(), language='spanish')\n","    sentiment_score = 0\n","\n","    for token in tokens:\n","        if token in emolex_lexicon:\n","            sentiment_score += emolex_lexicon[token]\n","\n","    return sentiment_score\n","\n","# Ruta del archivo Excel\n","ruta_archivo = \"/content/drive/MyDrive/Investigación/Data Entrenamiento.xlsx\"\n","\n","# Leer el archivo Excel y seleccionar la columna \"tweet\"\n","data = pd.read_excel(ruta_archivo)\n","column_name = 'tweet'\n","\n","# Cargar EmoLex\n","emolex_file = \"ruta_al_archivo_emolex.txt\"  # Reemplaza con la ruta al archivo descargado\n","emolex_lexicon = load_emolex(emolex_file)\n","\n","# Inicializar lista para almacenar las puntuaciones\n","emolex_scores = []\n","\n","for index, row in data.iterrows():\n","    text = row[column_name]\n","    emolex_score = analyze_emolex_sentiment(text, emolex_lexicon)\n","    emolex_scores.append(emolex_score)\n","\n","# Asignar las puntuaciones a la lista correspondiente\n","data['emolex_sentiment_score'] = emolex_scores\n","\n","# Guardar los resultados en un nuevo archivo Excel\n","result_file = \"/content/drive/MyDrive/Investigación/Resultados_Emolex_Sentimiento.xlsx\"\n","data.to_excel(result_file, index=False)\n","\n","print(\"Resultados de EmoLex guardados en:\", result_file)\n"],"metadata":{"id":"ES9DS2tvsg_j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Opinion Lexicon"],"metadata":{"id":"rfHHYfGjxbsw"}},{"cell_type":"code","source":["import pandas as pd\n","from nltk.corpus import opinion_lexicon\n","from nltk.tokenize import word_tokenize\n","import nltk\n","\n","# Descargar el recurso Spanish stopwords\n","nltk.download('stopwords')\n","\n","# Función para cargar Opinion Lexicon de NLTK\n","pos_words = set(opinion_lexicon.positive())\n","neg_words = set(opinion_lexicon.negative())\n","\n","# Función para analizar el sentimiento de un texto usando Opinion Lexicon\n","def analyze_opinion_lexicon_sentiment(text):\n","    tokens = word_tokenize(text.lower(), language='spanish')\n","    sentiment_score = 0\n","\n","    for token in tokens:\n","        if token in pos_words:\n","            sentiment_score += 1\n","        elif token in neg_words:\n","            sentiment_score -= 1\n","\n","    return sentiment_score\n","\n","# Ruta del archivo Excel\n","ruta_archivo = \"/content/drive/MyDrive/Investigación/Data Entrenamiento.xlsx\"\n","\n","# Leer el archivo Excel y seleccionar la columna \"tweet\"\n","data = pd.read_excel(ruta_archivo)\n","column_name = 'tweet'\n","\n","# Inicializar lista para almacenar las puntuaciones\n","opinion_lexicon_scores = []\n","\n","for index, row in data.iterrows():\n","    text = row[column_name]\n","    opinion_lexicon_score = analyze_opinion_lexicon_sentiment(text)\n","    opinion_lexicon_scores.append(opinion_lexicon_score)\n","\n","# Asignar las puntuaciones a la lista correspondiente\n","data['opinion_lexicon_sentiment_score'] = opinion_lexicon_scores\n","\n","# Guardar los resultados en un nuevo archivo Excel\n","result_file = \"/content/drive/MyDrive/Investigación/Resultados_Opinion_Lexicon_Sentimiento.xlsx\"\n","data.to_excel(result_file, index=False)\n","\n","print(\"Resultados del Opinion Lexicon guardados en:\", result_file)\n"],"metadata":{"id":"3ioCaomdxWdy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##NRC Emotion Lexicon"],"metadata":{"id":"cNY_Rk01vS69"}},{"cell_type":"code","source":["import pandas as pd\n","from nltk.tokenize import word_tokenize\n","import nltk\n","\n","# Descargar el recurso Spanish stopwords\n","nltk.download('stopwords')\n","\n","# Función para cargar el NRC Emotion Lexicon\n","def load_nrc_emotion_lexicon(nrc_emotion_file):\n","    nrc_emotion_lexicon = {}\n","    with open(nrc_emotion_file, 'r') as file:\n","        for line in file:\n","            word, emotion, value = line.strip().split()\n","            if emotion not in nrc_emotion_lexicon:\n","                nrc_emotion_lexicon[emotion] = {}\n","            nrc_emotion_lexicon[emotion][word] = int(value)\n","    return nrc_emotion_lexicon\n","\n","# Función para analizar el sentimiento de un texto usando NRC Emotion Lexicon\n","def analyze_nrc_emotion_sentiment(text, nrc_emotion_lexicon):\n","    tokens = word_tokenize(text.lower(), language='spanish')\n","    emotion_scores = {emotion: 0 for emotion in nrc_emotion_lexicon.keys()}\n","\n","    for token in tokens:\n","        for emotion in nrc_emotion_lexicon.keys():\n","            if token in nrc_emotion_lexicon[emotion]:\n","                emotion_scores[emotion] += nrc_emotion_lexicon[emotion][token]\n","\n","    return emotion_scores\n","\n","# Ruta del archivo Excel\n","ruta_archivo = \"/content/drive/MyDrive/Investigación/Data Entrenamiento.xlsx\"\n","\n","# Leer el archivo Excel y seleccionar la columna \"tweet\"\n","data = pd.read_excel(ruta_archivo)\n","column_name = 'tweet'\n","\n","# Cargar NRC Emotion Lexicon\n","nrc_emotion_file = \"ruta_al_archivo_nrc_emotion_lexicon.txt\"  # Reemplaza con la ruta al archivo NRC Emotion Lexicon descargado\n","nrc_emotion_lexicon = load_nrc_emotion_lexicon(nrc_emotion_file)\n","\n","# Inicializar lista para almacenar las puntuaciones de emociones\n","emotion_scores_list = []\n","\n","for index, row in data.iterrows():\n","    text = row[column_name]\n","    emotion_scores = analyze_nrc_emotion_sentiment(text, nrc_emotion_lexicon)\n","    emotion_scores_list.append(emotion_scores)\n","\n","# Crear un DataFrame de las puntuaciones de emociones\n","emotion_df = pd.DataFrame(emotion_scores_list)\n","\n","# Asignar las puntuaciones a las listas correspondientes\n","for emotion in emotion_df.columns:\n","    data[f'nrc_emotion_{emotion}_score'] = emotion_df[emotion]\n","\n","# Guardar los resultados en un nuevo archivo Excel\n","result_file = \"/content/drive/MyDrive/Investigación/Resultados_NRC_Emotion_Sentimiento.xlsx\"\n","data.to_excel(result_file, index=False)\n","\n","print(\"Resultados del NRC Emotion Lexicon guardados en:\", result_file)\n"],"metadata":{"id":"Gh8c4rzOvYNN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#BERT\n","Bidirectional Encoder Representations from Transformers\n"],"metadata":{"id":"-duWOU6ZrJqJ"}},{"cell_type":"code","source":["import pandas as pd\n","from transformers import BertTokenizer, BertForSequenceClassification\n","import torch\n","\n","# Cargar el conjunto de datos desde un archivo Excel\n","ruta_archivo = \"/content/drive/MyDrive/Investigación/Data Entrenamiento.xlsx\"\n","data = pd.read_excel(ruta_archivo)\n","\n","# Ajustar los valores de sentimiento al rango de -1 a 1\n","data['sentimiento1'] = ((data['sentimiento1'] - 2) / 2).astype(float)\n","\n","# Cargar el tokenizador y el modelo preentrenado de BERT\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n","\n","# Poner el modelo en modo de evaluación\n","model.eval()\n","\n","# Iterar a través de cada tweet y realizar predicciones\n","bert_sentiment_scores = []  # Lista para almacenar las predicciones de BERT\n","\n","for i, row in data.iterrows():\n","    tweet = row['tweet']\n","    # Tokenizar y convertir el tweet en tensor\n","    inputs = tokenizer(tweet, return_tensors=\"pt\", padding=True, truncation=True)\n","    input_ids = inputs['input_ids']\n","    attention_mask = inputs['attention_mask']\n","\n","    # Realizar la predicción\n","    with torch.no_grad():\n","        output = model(input_ids, attention_mask=attention_mask)[0]\n","\n","    # Escalar la predicción al rango de -1 a 1\n","    prediction = output.item()\n","    scaled_prediction = 2 * (prediction - min_prediction) / (max_prediction - min_prediction) - 1\n","\n","    bert_sentiment_scores.append(scaled_prediction)  # Agregar la predicción escalada a la lista\n","\n","# Crear una nueva columna en el DataFrame para las puntuaciones de BERT\n","data['bert_sentiment_score_s'] = bert_sentiment_scores\n","\n","# Guardar los resultados en un nuevo archivo Excel\n","result_file = \"/content/drive/MyDrive/Investigación/Resultados_Sentimiento.xlsx\"\n","data.to_excel(result_file, index=False)\n","\n","print(\"Resultados guardados en:\", result_file)\n","\n"],"metadata":{"id":"SozBa7TR4vWc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694798280914,"user_tz":300,"elapsed":4485,"user":{"displayName":"Gabriel Orosco Jiménez","userId":"17287692603456795875"}},"outputId":"8a8cf424-4bc5-453c-e22e-3cf8d5fe9c57"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Resultados guardados en: /content/drive/MyDrive/Investigación/Resultados_Sentimiento.xlsx\n"]}]},{"cell_type":"markdown","source":["#Transformadores"],"metadata":{"id":"zM4559OMluMd"}},{"cell_type":"code","source":["!pip install transformers\n","!pip install matplotlib\n","!pip install textblob\n","!pip install xformers"],"metadata":{"id":"awbBnhluyKyw"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uxRC0FGhxs4M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694798669628,"user_tz":300,"elapsed":6993,"user":{"displayName":"Gabriel Orosco Jiménez","userId":"17287692603456795875"}},"outputId":"d00faad9-7736-4ca4-c5f1-7a9e2d2e4932"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Resultados guardados en: /content/drive/MyDrive/Investigación/Resultados_Sentimiento.xlsx\n"]}],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from torch.utils.data import DataLoader, TensorDataset\n","import torch\n","\n","# Cargar el conjunto de datos desde un archivo Excel\n","ruta_archivo = \"/content/drive/MyDrive/Investigación/Data Entrenamiento.xlsx\"\n","data = pd.read_excel(ruta_archivo)\n","\n","# Cargar el tokenizador y el modelo preentrenado de BERT\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)  # Usar num_labels=1 para una sola unidad de clasificación\n","\n","# Definir DataLoader para entrenamiento y prueba\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32)\n","\n","# Definir función de pérdida y optimizador\n","criterion = torch.nn.MSELoss()  # Usar MSE (Mean Squared Error) para la regresión\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n","\n","# Resto del código (entrenar y evaluar el modelo)...\n","\n","# Poner el modelo en modo de evaluación\n","model.eval()\n","\n","# Iterar a través de cada tweet y realizar predicciones\n","bert_sentiment_scores = []  # Lista para almacenar las predicciones de BERT\n","\n","for i, row in data.iterrows():\n","    tweet = row['tweet']\n","    # Tokenizar y convertir el tweet en tensor\n","    inputs = tokenizer(tweet, return_tensors=\"pt\", padding=True, truncation=True)\n","    input_ids = inputs['input_ids']\n","    attention_mask = inputs['attention_mask']\n","\n","    # Realizar la predicción\n","    with torch.no_grad():\n","        output = model(input_ids, attention_mask=attention_mask).logits.squeeze().item()  # Usar .logits y .squeeze() para obtener un valor escalar\n","\n","    bert_sentiment_scores.append(output)  # Agregar la predicción a la lista\n","\n","# Asegurarnos de que las puntuaciones tengan la misma longitud que el conjunto de datos\n","assert len(bert_sentiment_scores) == len(data)\n","\n","# Agregar las puntuaciones de sentimiento de BERT a una nueva columna\n","data['bert_sentiment_score'] = bert_sentiment_scores\n","\n","# Guardar los resultados en un nuevo archivo Excel\n","result_file = \"/content/drive/MyDrive/Investigación/Resultados_Sentimiento.xlsx\"\n","data.to_excel(result_file, index=False)\n","\n","print(\"Resultados guardados en:\", result_file)\n"]},{"cell_type":"markdown","source":["## RoBERTa\n","A Robustly Optimized BERT Pretraining Approach\n","\n"],"metadata":{"id":"XR-EiDU5BMw3"}},{"cell_type":"code","source":["# Instala las bibliotecas si aún no las tienes instaladas\n","# pip install transformers\n","# pip install torch\n","\n","import torch\n","from transformers import RobertaTokenizer, RobertaForSequenceClassification\n","\n","# Carga el modelo preentrenado y el tokenizador\n","model_name = \"roberta-base\"  # Puedes cambiar el modelo según tus necesidades\n","tokenizer = RobertaTokenizer.from_pretrained(model_name)\n","model = RobertaForSequenceClassification.from_pretrained(model_name)\n","\n","# Define el texto que deseas analizar\n","texto = \"Este es un ejemplo de análisis de sentimiento.\"\n","\n","# Tokeniza el texto y lo convierte en un tensor\n","inputs = tokenizer(texto, return_tensors=\"pt\", padding=True, truncation=True)\n","\n","# Realiza la clasificación de sentimiento\n","with torch.no_grad():\n","    outputs = model(**inputs)\n","\n","# Obtiene las probabilidades de clasificación\n","logits = outputs.logits\n","\n","# Convierte las probabilidades en una etiqueta de sentimiento\n","predicted_class = torch.argmax(logits, dim=1).item()\n","\n","# Define una lista de etiquetas de sentimiento\n","etiquetas_sentimiento = [\"Negativo\", \"Neutral\", \"Positivo\"]\n","\n","# Muestra el resultado\n","print(f\"Sentimiento: {etiquetas_sentimiento[predicted_class]}\")\n"],"metadata":{"id":"kde_KD7XBdsI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##DistilBERT\n"],"metadata":{"id":"iK6ytlc2BxKE"}},{"cell_type":"code","source":["# Instala las bibliotecas si aún no las tienes instaladas\n","# pip install transformers\n","# pip install torch\n","\n","import torch\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","\n","# Carga el modelo preentrenado y el tokenizador\n","model_name = \"distilbert-base-uncased\"  # Puedes cambiar el modelo según tus necesidades\n","tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n","model = DistilBertForSequenceClassification.from_pretrained(model_name)\n","\n","# Define el texto que deseas analizar\n","texto = \"Este es un ejemplo de análisis de sentimiento.\"\n","\n","# Tokeniza el texto y lo convierte en un tensor\n","inputs = tokenizer(texto, return_tensors=\"pt\", padding=True, truncation=True)\n","\n","# Realiza la clasificación de sentimiento\n","with torch.no_grad():\n","    outputs = model(**inputs)\n","\n","# Obtiene las probabilidades de clasificación\n","logits = outputs.logits\n","\n","# Convierte las probabilidades en una etiqueta de sentimiento\n","predicted_class = torch.argmax(logits, dim=1).item()\n","\n","# Define una lista de etiquetas de sentimiento\n","etiquetas_sentimiento = [\"Negativo\", \"Neutral\", \"Positivo\"]\n","\n","# Muestra el resultado\n","print(f\"Sentimiento: {etiquetas_sentimiento[predicted_class]}\")\n"],"metadata":{"id":"TwFZfkfLBvfg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##GPT\n","\n","(Generative Pre-trained Transformer)"],"metadata":{"id":"Wu1tDHFPCAxS"}},{"cell_type":"code","source":["# Instala las bibliotecas si aún no las tienes instaladas\n","# pip install transformers\n","# pip install torch\n","\n","import torch\n","from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n","\n","# Carga el tokenizador y el modelo preentrenado GPT-2\n","model_name = \"gpt2\"  # Puedes cambiar el modelo según tus necesidades\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","model = GPT2ForSequenceClassification.from_pretrained(model_name)\n","\n","# Define el texto que deseas analizar\n","texto = \"Este es un ejemplo de análisis de sentimiento.\"\n","\n","# Tokeniza el texto y lo convierte en un tensor\n","inputs = tokenizer(texto, return_tensors=\"pt\", padding=True, truncation=True)\n","\n","# Realiza la clasificación de sentimiento\n","with torch.no_grad():\n","    outputs = model(**inputs)\n","\n","# Obtiene las probabilidades de clasificación\n","logits = outputs.logits\n","\n","# Convierte las probabilidades en una etiqueta de sentimiento\n","predicted_class = torch.argmax(logits, dim=1).item()\n","\n","# Define una lista de etiquetas de sentimiento\n","etiquetas_sentimiento = [\"Negativo\", \"Neutral\", \"Positivo\"]\n","\n","# Muestra el resultado\n","print(f\"Sentimiento: {etiquetas_sentimiento[predicted_class]}\")\n"],"metadata":{"id":"WI-aa3HiCFkv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##XLNet\n"],"metadata":{"id":"pOctLMOXCf82"}},{"cell_type":"code","source":["# Instala las bibliotecas si aún no las tienes instaladas\n","# pip install transformers\n","# pip install torch\n","\n","import torch\n","from transformers import XLNetTokenizer, XLNetForSequenceClassification\n","\n","# Carga el modelo preentrenado y el tokenizador\n","model_name = \"xlnet-base-cased\"  # Puedes cambiar el modelo según tus necesidades\n","tokenizer = XLNetTokenizer.from_pretrained(model_name)\n","model = XLNetForSequenceClassification.from_pretrained(model_name)\n","\n","# Define el texto que deseas analizar\n","texto = \"Este es un ejemplo de análisis de sentimiento.\"\n","\n","# Tokeniza el texto y lo convierte en un tensor\n","inputs = tokenizer(texto, return_tensors=\"pt\", padding=True, truncation=True)\n","\n","# Realiza la clasificación de sentimiento\n","with torch.no_grad():\n","    outputs = model(**inputs)\n","\n","# Obtiene las probabilidades de clasificación\n","logits = outputs.logits\n","\n","# Convierte las probabilidades en una etiqueta de sentimiento\n","predicted_class = torch.argmax(logits, dim=1).item()\n","\n","# Define una lista de etiquetas de sentimiento\n","etiquetas_sentimiento = [\"Negativo\", \"Neutral\", \"Positivo\"]\n","\n","# Muestra el resultado\n","print(f\"Sentimiento: {etiquetas_sentimiento[predicted_class]}\")\n"],"metadata":{"id":"6pwVMcYnCdlx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##T5\n","(Text-To-Text Transfer Transformer)"],"metadata":{"id":"stkKmlOZCsbi"}},{"cell_type":"code","source":["# Instala las bibliotecas si aún no las tienes instaladas\n","# pip install transformers\n","# pip install torch\n","\n","import torch\n","from transformers import T5Tokenizer, T5ForConditionalGeneration\n","\n","# Carga el modelo preentrenado y el tokenizador\n","model_name = \"t5-small\"  # Puedes cambiar el modelo según tus necesidades\n","tokenizer = T5Tokenizer.from_pretrained(model_name)\n","model = T5ForConditionalGeneration.from_pretrained(model_name)\n","\n","# Define el texto que deseas analizar\n","texto = \"Este es un ejemplo de análisis de sentimiento.\"\n","\n","# Formato de entrada y salida para T5\n","input_text = \"sentiment: \" + texto + \" </s>\"\n","\n","# Tokeniza el texto y lo convierte en un tensor\n","input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n","\n","# Realiza la generación de texto para obtener la etiqueta de sentimiento\n","with torch.no_grad():\n","    output = model.generate(input_ids, max_length=2)\n","\n","# Convierte la salida en texto\n","predicted_label = tokenizer.decode(output[0], skip_special_tokens=True)\n","\n","# Define una lista de etiquetas de sentimiento\n","etiquetas_sentimiento = [\"Negativo\", \"Neutral\", \"Positivo\"]\n","\n","# Muestra el resultado\n","print(f\"Sentimiento: {etiquetas_sentimiento[int(predicted_label)]}\")\n"],"metadata":{"id":"n-vmgH5xCyf2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##ELECTRA\n","(Efficiently Learning an Encoder that Classifies Token Replacements Accurately)"],"metadata":{"id":"wctLeZPVDHKg"}},{"cell_type":"code","source":["# Instala las bibliotecas si aún no las tienes instaladas\n","# pip install transformers\n","# pip install torch\n","\n","import torch\n","from transformers import ElectraTokenizer, ElectraForSequenceClassification\n","\n","# Carga el modelo preentrenado y el tokenizador\n","model_name = \"google/electra-base-discriminator\"  # Puedes cambiar el modelo según tus necesidades\n","tokenizer = ElectraTokenizer.from_pretrained(model_name)\n","model = ElectraForSequenceClassification.from_pretrained(model_name)\n","\n","# Define el texto que deseas analizar\n","texto = \"Este es un ejemplo de análisis de sentimiento.\"\n","\n","# Tokeniza el texto y lo convierte en un tensor\n","inputs = tokenizer(texto, return_tensors=\"pt\", padding=True, truncation=True)\n","\n","# Realiza la clasificación de sentimiento\n","with torch.no_grad():\n","    outputs = model(**inputs)\n","\n","# Obtiene las probabilidades de clasificación\n","logits = outputs.logits\n","\n","# Convierte las probabilidades en una etiqueta de sentimiento\n","predicted_class = torch.argmax(logits, dim=1).item()\n","\n","# Define una lista de etiquetas de sentimiento\n","etiquetas_sentimiento = [\"Negativo\", \"Neutral\", \"Positivo\"]\n","\n","# Muestra el resultado\n","print(f\"Sentimiento: {etiquetas_sentimiento[predicted_class]}\")\n"],"metadata":{"id":"j1Waks-2DJPt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##BART (BART: Denoising Sequence-to-Sequence Pretraining for Natural Language Generation, Translation, and Comprehension)"],"metadata":{"id":"NDtsEE14DR9c"}},{"cell_type":"code","source":["# Instala las bibliotecas si aún no las tienes instaladas\n","# pip install transformers\n","# pip install torch\n","\n","import torch\n","from transformers import BartTokenizer, BartForConditionalGeneration\n","\n","# Carga el modelo preentrenado y el tokenizador\n","model_name = \"facebook/bart-large-cnn\"  # Puedes cambiar el modelo según tus necesidades\n","tokenizer = BartTokenizer.from_pretrained(model_name)\n","model = BartForConditionalGeneration.from_pretrained(model_name)\n","\n","# Define el texto que deseas analizar\n","texto = \"Este es un ejemplo de análisis de sentimiento.\"\n","\n","# Formato de entrada y salida para BART\n","input_text = \"sentiment: \" + texto + \"</s>\"\n","\n","# Tokeniza el texto y lo convierte en un tensor\n","input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n","\n","# Realiza la generación de texto para obtener la etiqueta de sentimiento\n","with torch.no_grad():\n","    output = model.generate(input_ids, max_length=2)\n","\n","# Convierte la salida en texto\n","predicted_label = tokenizer.decode(output[0], skip_special_tokens=True)\n","\n","# Define una lista de etiquetas de sentimiento\n","etiquetas_sentimiento = [\"Negativo\", \"Neutral\", \"Positivo\"]\n","\n","# Muestra el resultado\n","print(f\"Sentimiento: {etiquetas_sentimiento[int(predicted_label)]}\")\n"],"metadata":{"id":"DzsBJMa7Djnj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##CTRL (Conditional Transformer Language Model)"],"metadata":{"id":"WJLlS6hND1bA"}},{"cell_type":"code","source":["# Instala las bibliotecas si aún no las tienes instaladas\n","# pip install transformers\n","# pip install torch\n","\n","import torch\n","from transformers import CTRLTokenizer, CTRLModel\n","\n","# Carga el modelo preentrenado y el tokenizador\n","model_name = \"SalesforceCTRL/ctrl\"  # Puedes cambiar el modelo según tus necesidades\n","tokenizer = CTRLTokenizer.from_pretrained(model_name)\n","model = CTRLModel.from_pretrained(model_name)\n","\n","# Define el texto que deseas analizar\n","texto = \"Este es un ejemplo de análisis de sentimiento.\"\n","\n","# Tokeniza el texto y lo convierte en un tensor\n","inputs = tokenizer(texto, return_tensors=\"pt\", padding=True, truncation=True)\n","\n","# Realiza la clasificación de sentimiento\n","with torch.no_grad():\n","    outputs = model(**inputs)\n","\n","# Puedes procesar los outputs como desees, ya que CTRL no está diseñado para tareas específicas de clasificación de sentimiento.\n","\n","# Muestra el resultado\n","print(outputs)\n"],"metadata":{"id":"8-4RHug0D4RV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##GloVe (Global Vectors for Word Representation)"],"metadata":{"id":"UEZRTFXSKGKm"}},{"cell_type":"code","source":["# Carga los vectores GloVe preentrenados\n","word_vectors = {}\n","glove_file = 'glove.6B.50d.txt'  # Ruta al archivo GloVe que descargaste\n","with open(glove_file, 'r', encoding='utf-8') as file:\n","    for line in file:\n","        values = line.split()\n","        word = values[0]\n","        vector = [float(val) for val in values[1:]]\n","        word_vectors[word] = vector\n","\n","# Define una función para calcular el sentimiento promedio de un texto\n","def calculate_sentiment(text):\n","    words = text.split()\n","    sentiment_vector = [0.0] * len(word_vectors['the'])  # Inicializa un vector de sentimiento\n","\n","    for word in words:\n","        if word in word_vectors:\n","            word_vector = word_vectors[word]\n","            sentiment_vector = [a + b for a, b in zip(sentiment_vector, word_vector)]\n","\n","    if len(words) > 0:\n","        sentiment_vector = [x / len(words) for x in sentiment_vector]\n","\n","    return sentiment_vector\n","\n","# Ejemplo de uso\n","texto = \"La economía está creciendo y las inversiones están aumentando.\"\n","sentiment_vector = calculate_sentiment(texto)\n","\n","# Define una polaridad de sentimiento basada en el vector resultante\n","def determine_sentiment(polarity_vector):\n","    if polarity_vector[0] > 0.2:\n","        return \"Positivo\"\n","    elif polarity_vector[0] < -0.2:\n","        return \"Negativo\"\n","    else:\n","        return \"Neutral\"\n","\n","sentimiento = determine_sentiment(sentiment_vector)\n","\n","# Muestra el resultado\n","print(f\"Texto: '{texto}'\")\n","print(f\"Sentimiento: {sentimiento}\")\n"],"metadata":{"id":"IHpjtohwKDex"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Redes Neuronales"],"metadata":{"id":"05MFw7NwMTqH"}},{"cell_type":"markdown","source":["## Red neuronal recurrente (RNN)"],"metadata":{"id":"bMkusDCSMaGl"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","\n","# Datos de ejemplo: textos y etiquetas de sentimiento\n","textos = [\"Este es un buen producto.\",\n","          \"No me gusta esta película en absoluto.\",\n","          \"La comida en este restaurante es deliciosa.\",\n","          \"El servicio al cliente fue terrible.\",\n","          \"Me siento muy feliz hoy.\"]\n","\n","etiquetas = [1, 0, 1, 0, 1]  # 1 para sentimiento positivo, 0 para sentimiento negativo\n","\n","# Tokenización y secuencias\n","tokenizer = Tokenizer(num_words=1000, oov_token='<OOV>')  # Limita el vocabulario a 1000 palabras\n","tokenizer.fit_on_texts(textos)\n","secuencias = tokenizer.texts_to_sequences(textos)\n","secuencias_padded = pad_sequences(secuencias, maxlen=10, padding='post', truncating='post')\n","\n","# Creación del modelo\n","model = Sequential()\n","model.add(Embedding(input_dim=1000, output_dim=16, input_length=10))\n","model.add(LSTM(32))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Entrenamiento del modelo\n","model.fit(secuencias_padded, etiquetas, epochs=10, verbose=2)\n","\n","# Ejemplo de clasificación de sentimiento\n","nuevo_texto = [\"Me encanta este libro.\"]\n","secuencia_nuevo_texto = tokenizer.texts_to_sequences(nuevo_texto)\n","secuencia_nuevo_texto_padded = pad_sequences(secuencia_nuevo_texto, maxlen=10, padding='post', truncating='post')\n","\n","resultado = model.predict(secuencia_nuevo_texto_padded)\n","\n","if resultado[0] >= 0.5:\n","    print(\"Sentimiento positivo\")\n","else:\n","    print(\"Sentimiento negativo\")\n"],"metadata":{"id":"Yd81JRJhMYbe"},"execution_count":null,"outputs":[]}]}
